# AltNet
An Old Alternative, tried again
<hr>
</br></br>
&nbsp; &nbsp; &nbsp; To network within a hash is to plan or dynamically prepare ahead. Hash-based Networking isn't a new concept and there has been in many cases prooving this as a possible ventrue for seperating the signature from the entity controller.</br>
&nbsp; &nbsp; &nbsp; Hash networking in the past was slow and incapable of handling rapid growth but through services like Bitcoin, some of the slowest hashed-networks are now some of the most relieable data-storage networks (even though it's barely being used as data-storage for external data). Theoretically it should be possible to encode a network within a hash then use other hashes to then interact with that hashed-network. So in other words, you should be able to place a hash within a network like bitcoin and then when opening that hash we should be able to have a method of interacting. There after, everything should work as intended. Perhaps you could say there needs to be unpacking time for the hash can have any number of hashes within it. Just from this, it should be easy to see why old hash networks failed. Infinitely possible hashes makes more work to unpack, but much has happened since 2009.</br></br>
&nbsp; -&nbsp; &nbsp; Bitcoin proved hash networks can work.</br></br>
&nbsp; &nbsp; -&nbsp; &nbsp; IPFS proved hash networks can interact.</br></br>
&nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; IPNS proved hash networks can direct.</br></br>
&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; Itty-Bitty-Site proved hash networks can be self-served & portable.</br></br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; PvtPpr proved hash networks can self-direct data to-and-from any network to any network with or without being apart of the hash network.
</br></br></br>
&nbsp; &nbsp; &nbsp; To self host your data, is to be upholding your end of the internet. If we consider the internet any-and-all interconnected services working as one greater entity, then hash networks are the portable bays of the internet, or at least they can be. Many people consider the internet anything they can bring up on a browser & in this state, it's not completely wrong but a browser is a limited technology in retrospect that has tons of extra features to make more be able to appear within it so it can be altered by a user to see more networks or more networks can just work within the link-provisions.</br>
&nbsp; &nbsp; &nbsp; Itty-Bitty-Site took the basic link-provisions and figured out how to work around basic storage by hashing the data and stuffing that hash within the link. This does two major things. First, it separates data-storage from the server or harddrive. Second, it separates servers from workload from permissions.</br>
&nbsp; &nbsp; &nbsp; Seperating data-storage from the server or drive may seem like an overshoot because sure something is storing the text file of the link but no drive or server is storing the data within the Itty-Bitty-Site Hashlink but simply the text of the link containing the hash. This means that data can actually direct the browser while the node-server is only decoupling the hash from it's compression. This is important because this coupling can be modified and thus making some publicly available nodes unable to decouple the hash from it's compression but just knowing the difference (key) or having the decoding script will still open the modified hashlink. If the server was important or in this case if the node was important then no other node could decouple the hash. But because Itty-Bitty-Site separates front-link from slug, we can modify the front end and compression without messing up the data inside so no part of any part is reliant on any part except for itself & the decoding key.</br>
&nbsp; &nbsp; &nbsp; Seperating servers from workload was first introduced in the late 90s and early 2000s but nowadays there are tons of ways to perform this task. However, Itty-Bitty-Site takes this a step further by having essentially a ring-layering system within each hashlink, allowing the separation of permissions. So the node has ring0 capabilities like displaying and serving ads independently form the data or client ads. The data has ring1 which can dip into ring0, it is not easy unless the node allows for the extra execution of ring0. So even in how Itty-Bitty-Site is designed, the node can be independently different from others. For example, there was a full JavaScript based node for Itty-Bitty-Site known as Itty-Bitty-App which gave data ring0 access but didn't give up any permissions nor give any extra allowances, only allowed for CSS for ring1. Custom-Sheet-Styling is a type of permission for web-based systems because entire sites can be made form CSS just like entire sites can be made form HTML or JS. This node was successful enough for Itty-Bitty-Site to gain control of the node for a more unified experience. This did prove, that the server is similar to Bitcoin Nodes and Miners. The configuration, adaption, & execution is 100% up to the user just as long as it works with the rest.
</br></br>
&nbsp; &nbsp; &nbsp; At this point you are potentially seeing the problem, size and link length.  There isn't much you can place in a hashlink that can be easily distributed online as-of-right-now. For example some of the places online that allow for large link usage are link-shorteners.</br>
&nbsp; &nbsp; &nbsp; The biggest publicly available link-shortner will allow for up to 16,000 bytes of data, 16 kilobytes and that's it. However, what all can you place within 16KB? I placed a generator within 16KB, a generator that would allow anyone to form a hashlink to the Itty-Bitty-Site standards so any Itty-Bitty-Node could run them. At first I did a simple, "paste a link to see the image" generator that slowly taught me how to use Base64 media hashes as native media. That slowly opened up to PvtPpr generators that then allowed anyone whom can open the generator to build article pages, blogs, and similar top-down text-driven webpages. Slowly more generators was built, each doing something different from individual pages to newspapers to Non-Fungible Tokens to threading through block media for alternative entry points to-and-from network hashes, links & hashlinks. Eventually there became an alternative internet option. Dubbed "NaNo Networks" by an anonymous entity, these small little networks of generally different designs were starting to appear even outside PvtPpr & Itty-Bitty-Sites. PvtPpr took the blockchain idea and forced it inside the hashnet idea, so every PvtPpr generator also builds a blockchain type database within the hash. A data-determined pointer to the previous data-block. This does return to the problem we had before with hash networks thou.</br>
&nbsp; &nbsp; &nbsp; Bitcoin just like another similar network before it, they separated each hash from each other but instead used internal pointers to point to the previous data-block. This is similar to how your computer talks to internal parts nowadays like registered network addressing. This does allow for some amount of scaling and through the practice of "large block" chains, the bigger the block, the more difficult it may be to read, open & verify. There are tons of ways to address this problem, which the best is segmented hashing or naturally broken [into pieces] hashes. Now, looking back at what I did. I simply looped outside-in & inside-out again thusly allowing internal networking within a single hash that can talk & communicate externally on it's, technical, own.</br>
&nbsp; &nbsp; &nbsp; Now Itty-Bitty-Site (IB) has alternatives to compression some of which allows for even more compression then before. Unfortunately for myself, I am not able to figure any of the new compressions out. Honestly, most of it just sounds like off-shooting data for token swapping that makes IB v2 internet reliant instead of how IB v1 is an intranet on its own. I could very easily be wrong on this but because I can't seem to get IB v2 working for myself offline, I started to use IPFS & IPNS for extra help. At this point, I think IPFS & IPNS still isn't capable of handling temporary status figures like bitcoin can. For example, the PvtPpr IPNS & IPFS node had an error after the Helia update and has never been able to be recovered (although a new node is up running the same database), some of the original pointers don't point to the data in it's current location within it's own network. So IPNS won't direct to me because I"m a new entity or something and the original isn't found to verify or something, I'm not entirely sure. But from the problems of modern IPFS/IPNS & IB, I began a quest to a different option. A JavaScripted option.</br>
&nbsp; &nbsp; &nbsp; If you go through my repo (<a href="https://github.com/DigiMancer3D?tab=repositories" ref="no referrer noopener" target="_blank">DigiMancer3D</a>), you'll see some unusual builds like: <a href="https://github.com/DigiMancer3D/HashKey-Cutter" ref="noreferrer noopener" target="_blank">HashKey-Cutter</a>, <a href="https://github.com/DigiMancer3D/B64-Image-Compression" ref="noreferrer noopener" target="_blank">B64-Image-Compression</a>, <a href="https://github.com/DigiMancer3D/HashTeleporter" ref="noreferrer noopener" target="_blank">HashTeleporter</a>. I was specifically trying to find a method in JavaScript to remove data and be able to later replace that data without storing anything outside the token placed in the removal of data. I did also want to try to remove data without notice but that quickly died as I wanted to just get better compressions with less. I then tried various things and came up with Wave-Data API for <a href="https://github.com/DigiMancer3D/iCS" ref="noreferrer noopener" target="_blank">iCS</a>, an interactive Character Sheet for playing table-top & general RPGs. This was to help with the problems coming
 from D&D wanting to change their license agreement in a negative way. Wave-Data API allows anything on HTML/JS to be plug-and-play capable without losing important data. This allows for separation of individual units on a webpage or webapp from that webpage or webapp & it's controller/builder/running entity/foundation/host/provider/machine.</br>
&nbsp; &nbsp; &nbsp; Wave-Data API is designed where the data stored directs & informs the end system. For example, if you run iCS you can click on a button on the bottom of the page to get a hash that when inserted into iCS thorough it's input system, it will give you an item. This hash is one of a few to mix up the possible outcome of your item but there is a place to get a "Tutorial item" which then is a basic tutorial of the system for exporting and importing. The hash has upfront identifiers like ";," or ".,." that will be used to have different sections within the hash, natural break points that ID what's next. These IDs are set in two: Meta & Data. Meta ID means, what's next explains the data thereafter. Data ID means, this is the data with last said Meta. Not all Data needs a Meta thou. So if your system is private and you don't disclose Meta IDs in your wave data, you simply have to know what to look for. In some cases with iCS Wave-Data Hashed-APIs, there will be system movement scripts as data. Their Meta ID tells the system what button to press before inserting the Data but this only happens in very specific situations like placing the generated item in it's proper spot. For example you wouldn't want a weapon to show up in the armor area of your character, in other words: can't use a hammer as a arm guard and expect things to work the same. So, this does even allow for interactive Hashed-APIs, which sounds scary but since everything is CRC reversible, we can check to see what the script is ahead of time. If we know enough info about the webapp using this API, we could alter the script and even change names of items, etc etc etc.</br>
&nbsp; &nbsp; &nbsp; Being able to separate web objects like this use to be a very common thing but now we could do things like build an ad spot with a proper payment system outside the delivery network and for as long as the user is online, the ad should work completely as if it was original to that page even if the ad was injected using Wava-Data H-APIs even going to the point that the original ad-payee should still get paid accordingly as long as the ad-payer allows for embedding ad payments. This didn't seem to be helpful but it did allow anything on a webpage or webapp to be reduced down to an API now, which could reduce space in the hash. The idea here is, we could build a webpage that is dynamic but rigid enough to accept H-APIs to then build that webpage, webapp or even web-service only based on raw-data & it's Meta IDs but inline to the system that uses it. This is now the first step into dynamic compression without off shooting data, instead we are off shooting the un-important webcode to the opening system or web-de-complier-application.</br></br>
&nbsp; &nbsp; &nbsp; Having a method to arrange the data of a web-object to-and-fro the web-frame does mean if we can compress this H-API, we can make our web-systems even smaller while allowing for potentially more complicated native-web-features. Compression was next up. First I didn't even mean to go this route. I started attempting at building a new method of making this "Hashed-API" and wanted it to be well, a real hash and not just a Base64 Data-Blob. I want the end output to be a Base64 Data-Blob so we can hide in plain sight but also so the data within that Base64 Data-Blob will be treated like a Media Hash, allowing us to mix and match the output we'd like to have. So instead of having to predefined a video output or a text output, the output wouldn't really matter cause the web browser would be expecting web workers to assist. Well, that's exactly what we have, IB v1 web worker to be specific. Now we can setup the browser to expect any output in a manner that will allow us to launch almost anything from the server window that could even get us device ring0 access, theoretically at least.</br>
&nbsp; &nbsp; &nbsp; A bit of a stretch but I do mean that there are ways to access device data even to the device's ring0 vis-a-vis C++ through JavaScript. I wanted to add this in here because this is why we always need a generic but specific system of compression. IB v1 is a generic but specific compression that allows anyone to be able to open and inspect the data inside before launching. If we are allowing people to flash pull data off one site and natievly inject it back in somewhere else, someone could transfer a problem or Cross-Scripting attacks would become capable. Although it would be individual user attacking themsleves, it's possible to have a node attack users with this setup. This is why I drove myself to build the next best text-to-hash I could.</br></br>
&nbsp; &nbsp; &nbsp; What ever text-to-hash I build, it needs to be reversable even if it's not a CRC-hash. So over my trials, I've learned to use base JS to replace CRC-hashes completely. CRC-hashes are hashes made using maths that are reversable. Know the maths, and the rest just works. I want something that even if you know the maths or key, you still may not be able to open the hash or you may even without. Again, a stretch of thought but what if we took data and mixed it like a cryptocurrency-mixer. Tornado cash showed that with good enough ledgers, mixing cryptoic proofs could be done successfully for dynamic resolution payouts (the person getting the funds out doesn't have to be the person that put the funds in). So I took my shot at a talk-model based on a TV show.</br>
&nbsp; &nbsp; &nbsp; I came up with this insane talk-model or way for internal structures of an algorithm to communicate without sharing secrets across one another. The model was designed how the family in the hit TV sitcom, "Full House", communicated. Basically in short, the family was divided into groups: the eldest kid & her friend; the youngest two kids; the men; the only female adult. These groups all did different things but in turn overtime worked to solve the "riddle" or "mystery" set by the eldest kid & her friend. So here's the rundown: the eldest & her friend setup a riddle or mystery by using terms specific to their demographic & personal clique; the youngest two girls work as a complexity identifier (attempting to decode the slang used) while the youngest was used as a timeline tracker; the men get all the data from the youngest two seperately but none of the original data; the lone female adult is used a decoder being able to hear the near-correct but not correct references to pop culture near her personal clique & knowledge while also hearing the timeline to properly decode the message in a usable order. So, we have to identify complexities reference to an index lookup table then rebuild the original data to then read that data as intended.</br>
&nbsp; &nbsp; &nbsp; The talk-model became a lookup table but is a rule-guide for building these lookup tables by others. This takes complexities from 0 to 9 and maps them to a 4x4 grid with a centered +1 grid below the full grid to allign "to center". We then start based on the complexity to center and build upward on the z-axis while wrapping only to the right and turning only right to get a unassociated complex string helixly wrapped around an axis, aka data-DNA. This data-DNA can be reduced to a handfull of 0,A-D points as a "heatmap" which is what is returned by the lookup table when called, the heatmap. Along side the heatmap we do build an index of order which is also returned with the heatmap. 1 call, 2 returns. This allows us to add entropy into the process instead of only into the output but also inflates what we have (as entropy should do). Now we have to measure a few points, like lengths and weights of the input to help with decoding later and we slap that at the front. Before we continue, I needed a way to stack these hashes sometimes so we have a front-end indentifier which points to it's previous data-hash or iteslf. This will give us the first step in controlled compression.</br>
&nbsp; &nbsp; &nbsp; Setting up the talk model output to have Alphanumeric outputs normally with DIGIT-CHARACTER (in that order) paired allows for controlled token swapping without needing to record external tables or data. Simply, we have predetermined patterns that are made from a pattern table (something we can swap out as needed for unique compressions). The pattern table is simply the basic 11 binary outputs possible starting with 0. Then we take that table and dynamcially build other bigger patterns of higher complexity that can be easily compared to with the string in question needing compression. We swap these starting with the biggest at the highest number of possible loops down to just one instance of the highest number down to the lowest needed pattern length. Next we have a unqiue pattern set that is also systematically-swappable (to allow unique compresssions) that go through a similar pattern generation process to have fast swappable tokens like the basic binary patterns. Now we have a handful of arrays designed to handle generically specific patterns based on pre-determined swappable tables. So as long as those swappable tables are in the same order evertime, these patterns should generate the same every time. The arrays based on which was generated first, will be assigned a letter in a specific case. This letter is the array's address and will be used as such later.</br>
&nbsp; &nbsp; &nbsp; The generated patterns have addresses so we can build tokens based on this info in a way that should allow us to reverse the token. So the tokens we will be generating work a bit like a
 pick-and-place machine. So the first object in the token will be the length of the original data, followed by it's array address. If there is a additional letter in the oppisite case following "number_case-state-1-letter" then the third character in a token represents the number of loops this data was seen in a row. We do, however, limit to 26 or the length of the case-state-letter in it's alphabet. Using English puts at a 26 charcater limit and a 26 legnth-cap. The length-cap relys on all dynamic swappable data in token generation. So the first and last letters used can not be greater then what they are referencing, in this case the English alphabet. Since we are using a lower-case-letter for where in an array of patterns we are in, I kept that option down to essentially 9 options + "0" & "1". So all the arrays start "0", "1", then go from biggest pattern possible to smallest possible (in that array) so if 0 ever gets used, it'll be an obvious error outputing "0[array-address]0" or "1[array-address]A". The answer won't be obvious but knowing it's an error helps. Since the upper-case & lower-case arrays are used for counting, they start "0", "[A|a]", "[B|b]" and so on and so on. We don't want to add the "A|a" in case of counting because 1 can be assumed if no other pointer object is present. This will allow for swaps of 3->2, instead of only being able to go down to 4->3 with reducable results. This also means there is a chance for reductions like 26->2 & 26->3. Which are dramatic drops of difference without loosing technical data.</br>
&nbsp; &nbsp; &nbsp; So we build a slightly different setup to compress the hashing rows and the hashing index. The only difference is the pattern set used and when we look for single units in the loop for compression. We attempt to build dynamic patterns that should fit within a set of five arrays. So we end up with a strange but specific set of arrays where the length of them are not always the same but may range from 1 to 10 index lengths. The first 3 arrays are specific to wrapping the index specifically and only the index. The last 2 arrays are specific to removing the seperators of the hash-sections. The very last array is a backup-blaster (so to speak), where if there is some form of mishap missed, it should snagg it. There is an addressing array just like we had before for the only remaining alphabet characters, [o-s]. We need no characters before compression but we want to remove as much binary & trinary proofs, along side the break-point identifiers scattered in the graph, during compression. So the wrapper is going to attempt to abuse this idea of the only possible outputs are [a-n][o-z][A-Z], and from this it's pretty clear that we could potentially get to a numericaless hash. If we can get to that point, we could wrap once with numbers instead of characters to potentially shorten the end-result more to add in "normality" to the end-output.</br></br>
&nbsp; &nbsp; &nbsp; The wrapper compression is the last step and the most crucial. Up to this point we are taking potentially Trinary data with some extruding circumstances along with random-capable-numerical data to have a string of non-alphbet characters so when we add in the alphabet in to the alphanumerical string, this creates a break in the string that is machine recgonizable for decoding later, or at least that's the idea. We don't want any more arrays or tables to look through, so we can add our two base pattern arays and compress dynamically using Entangling Token Swaps.</br>
&nbsp; &nbsp; &nbsp; The largest section we can look at a single time is limited to 26, for the same reasons as ealier, the encoding language used is English with an alphabet limit of 26. For example, if you used Russian alphabet, the limit could be 33. If you used a language alhpabet that encompanied all the global alphabets with unique characters, the limit would be much larger. The point that we have a limited window doesn't really matter cause in the end we also have to determine if 26 is less-than, greater-than or half of the string needing to be compressed. We only need to view at the most at any time either our upper-most limit (in this case 26) or half the length of the string needing to be compressed, which ever is smallest. Now we look across the entirty of the string (ignoring empty or null sections after our limit is hit), recording all the possible tokens we see. Once we hit our lower-limit of 4, we subtract one from the upper-limit and repeat until both the lower & upper limits are equal. We only record new and unique tokens in the string. Then starting from the largest possible token, we swap all tokens we see in the array. When we build the token to swap with, the token ends up about 3-characters long and is set up like the following: [where the token-reference begins in the array (left-to-right)][how long the token-reference is][the number of loops this was seen in a row that was removed] with the output represented as [lower-case][upper-case][lower-case]. This can end up being similar potential output as before except if the loop times seen is greater than one. The system only adds the second lower-case character if the number of times it's seen is greater than one, so if we start by only looking for the possible tokens to swap with a set-loop for as many loops as we can fit into the array slowly working down to just the token, we can now get the biggest possible swaps every-single-time before the smallest possible swaps. This does allow us to loop through the array until a set number of times or until the system cannot find swaps to perform.</br>
&nbsp; &nbsp; &nbsp; Now we can take any string and if there's any repeatable pattern, we'll be able to swap the dupplicates for a decypherable token. In order for this to have the biggest possible outcome, we need to have as much of the data before wrapping, ready to be wrapped. That means the more similar our data is before compression and wrapping, the more we will be able to compress this way. So if anything, we need to look back and reconsider our data.</br></br>
&nbsp; &nbsp; &nbsp; When we are mixing up the data we take an extra step to consider how we are mixing the data. We are applying the character to an End-2-End Encrypted font called Node [DNDregular.otf]. This allows us to have a binary like state wihtout it being the binary-state of the input. We could, however, just use the binary-state of the input instead. As long as we still have a indexable way to record the input order any binary-like-state should work. Next we take the binary-state or binary-like-state and record the first row for each row & underside. Then we look at the next bit in each row and asks, "is this the same as the previous bit?" If so, it records a 1 in it's place. If no, it records a 0 in it's place. Making a "shifted" binary-state that is returnable to it's original by knowing the first bit (furthest to the left bit) is the real bit. At the same time, you can restucture the row during this point and as long as the inteded reciever knows where to move the real bit indicator, everything will work fine. Just always overflow back to 1 skipping 0 the first time around in these situations.







